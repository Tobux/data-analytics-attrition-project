---
title: "Group 8 - Identifying Attrition in Employees"
output: html_notebook
---

## 0. Prerequesits
```{r}
library(pROC)
library(dplyr)
library(tidyr)
library(outliers)
library(caret)
library(e1071)
library(randomForest)
library(xgboost)
```

## 1. Load the IBM-Dataset

Load the dataset and get some basic descriptions.
```{r}
attrition_data <- read.csv("Dataset-HR-Employee-Attrition.csv", stringsAsFactors = TRUE)

# Drop the EmployeeCount and EmployeeNumber columns as they add no information to the data
attrition_data <- attrition_data %>%
  select(-EmployeeCount, -EmployeeNumber, -StandardHours, -Over18)

summary(attrition_data)
```
### Correlation Matrix
```{r}
install.packages("corrplot")
library(corrplot)
```

```{r, fig.width=10, fig.height=10}
# Identify numeric columns
numeric_cols <- sapply(attrition_data, is.numeric)

# Create a new dataset with only numeric columns
numeric_data <- attrition_data[, numeric_cols]

# Compute the correlation matrix
cor_matrix <- cor(numeric_data)

# Plot the correlation matrix
corrplot(cor_matrix, method = "circle", tl.cex = 0.5)
```

### Outlier Detection 

```{r}
# Function to detect outliers using IQR method
iqr_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 4 * IQR
  upper_bound <- Q3 + 4 * IQR
  outliers <- x < lower_bound | x > upper_bound
  return(outliers)
}

# Apply the IQR outlier detection to numerical columns
numerical_columns <- attrition_data %>% select(where(is.numeric))
iqr_outliers_result <- sapply(numerical_columns, iqr_outliers)

# Count the number of outliers in each column
outliers_count <- colSums(iqr_outliers_result)

# Print the count of outliers for each numerical column
outliers_count

# Combine results with original data to show rows with outliers
attrition_data_with_iqr_outliers <- attrition_data %>%
  mutate(across(where(is.numeric), iqr_outliers, .names = "outlier_{col}"))

# Filter rows with any outliers
rows_with_iqr_outliers <- attrition_data_with_iqr_outliers %>%
  filter(if_any(starts_with("outlier_"), ~ . == TRUE))

# Print the number of rows with at least one outlier
num_rows_with_outliers <- nrow(rows_with_iqr_outliers)
cat("Number of rows with at least one outlier:", num_rows_with_outliers, " / ", num_rows_with_outliers/1470*100, "% \n")
```

```{r}
# Function to detect outliers using Z-score method
z_score_outliers <- function(x, threshold = 4) {
  z_scores <- scale(x)
  outliers <- abs(z_scores) > threshold
  return(outliers)
}

# Apply the Z-score outlier detection to numerical columns
z_score_outliers_result <- sapply(numerical_columns, z_score_outliers)

# Count the number of outliers in each column
z_outliers_count <- colSums(z_score_outliers_result)

# Print the count of outliers for each numerical column
z_outliers_count

# Combine results with original data to show rows with outliers
attrition_data_with_z_outliers <- attrition_data %>%
  mutate(across(where(is.numeric), z_score_outliers, .names = "outlier_{col}"))

# Filter rows with any outliers
rows_with_z_outliers <- attrition_data_with_z_outliers %>%
  filter(if_any(starts_with("outlier_"), ~ . == TRUE))

# Print the number of rows with at least one outlier
num_rows_with_z_outliers <- nrow(rows_with_z_outliers)
cat("Number of rows with at least one Z-score outlier:", num_rows_with_z_outliers, " / ", num_rows_with_z_outliers/1470*100, "% \n")

# Display rows with outliers
rows_with_z_outliers

```

We will proceed with all data as a closer look on the identified outliers by the z-Score method revealed only high, but reasonable years of working with one company.

### Preprocessing

We want to scale our numerical features to have equal influences on the models and encode our categorical features.

```{r}
# Scale numerical columns excluding Attrition
preprocess_params <- preProcess(attrition_data %>% select(where(is.numeric), -Attrition), method = c("center", "scale"))
scaled_numerical <- predict(preprocess_params, attrition_data %>% select(where(is.numeric)))

# Add the scaled numerical columns back to the dataset
attrition_data <- attrition_data %>%
  select(-where(is.numeric)) %>%
  bind_cols(scaled_numerical)

# One-hot encoding for categorical columns (excluding Attrition)
categorical_columns <- attrition_data %>% select(where(is.factor), -Attrition)
encoded_categorical <- model.matrix(~ . - 1, data = categorical_columns) %>% as.data.frame()

# Remove original categorical columns and add the encoded ones
attrition_data <- attrition_data %>%
  select(-one_of(names(categorical_columns))) %>%
  bind_cols(encoded_categorical)

# Ensure the Attrition column is retained
attrition_data <- attrition_data %>%
  select(-Attrition) %>%
  bind_cols(attrition_data %>% select(Attrition))
```

### Create Stratified Train-Test-Split
```{r}
set.seed(142)  # For reproducibility
train_index <- createDataPartition(attrition_data$Attrition, p = 0.8, list = FALSE)
train_data <- attrition_data[train_index, ]
test_data <- attrition_data[-train_index, ]

# Ensure 'Attrition' is a factor and has the correct levels
train_data$Attrition <- as.factor(train_data$Attrition)

# Clean column names to ensure they are syntactically valid
colnames(train_data) <- make.names(colnames(train_data))
colnames(test_data) <- make.names(colnames(test_data))

```

### Nested Cross-Validation and Hyperparameter-Tuning (based on Accuracy)

```{r}
# Define the control function for cross-validation
outer_cv <- trainControl(method = "cv", number = 5, savePredictions = "final", classProbs = TRUE)

# Define the parameter grids for each model
param_grids <- list(
  knn = expand.grid(kmax = seq(3, 21, by = 2), distance = 2, kernel = "optimal"),
  rpart = expand.grid(cp = seq(0.01, 0.1, by = 0.01)),
  glm = NULL,
  rf = expand.grid(mtry = seq(2, 10, by = 2)),
  svm = expand.grid(C = 2^(2:4), sigma = 2^(-1:-3))
)

# Models list with method names as used in caret package
models <- list(
  knn = "kknn",
  rpart = "rpart",
  glm = "glm",
  rf = "rf",
  svm = "svmRadial"
)

# Perform nested cross-validation
results <- list()
for (model_name in names(models)) {
  model_method <- models[[model_name]]
  param_grid <- param_grids[[model_name]]
  
  # Perform nested cross-validation
  set.seed(123)
  nested_cv <- train(
    Attrition ~ ., data = train_data,
    method = model_method,
    tuneGrid = param_grid,
    trControl = outer_cv,
    metric = "Accuracy"
  )
  
  results[[model_name]] <- nested_cv
}

# Display results
results


```
### Evaluation

```{r}
# Evaluate models on test data
evaluation_results <- list()

for (model_name in names(results)) {
  model <- results[[model_name]]
  
  # Generate predictions
  predictions <- predict(model, newdata = test_data)
  probabilities <- predict(model, newdata = test_data, type = "prob")[, 2]
  
  # Confusion matrix
  cm <- confusionMatrix(predictions, test_data$Attrition)
  print(paste("Confusion Matrix for", model_name))
  print(cm)
  
  # ROC curve
  roc_curve <- roc(test_data$Attrition, probabilities)
  print(paste("AUC for", model_name))
  print(auc(roc_curve))
  
  # Plot ROC curve
  plot(roc_curve, main = paste("ROC Curve for", model_name))
  
  # Store results
  evaluation_results[[model_name]] <- list(confusion_matrix = cm, roc_curve = roc_curve)
}

# Display results
evaluation_results
```

### Re-Optimization for Recall

```{r}
# Define the control function for cross-validation
outer_cv <- trainControl(
  method = "cv",
  number = 5,
  savePredictions = "final",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Define the parameter grids for each model
param_grids <- list(
  knn = expand.grid(kmax = seq(3, 21, by = 2), distance = 2, kernel = "optimal"),
  rpart = expand.grid(cp = seq(0.01, 0.1, by = 0.01)),
  glm = NULL,
  rf = expand.grid(mtry = seq(2, 10, by = 2)),
  svm = expand.grid(C = 2^(2:4), sigma = 2^(-1:-3))
)

# Models list with method names as used in caret package
models <- list(
  knn = "kknn",
  rpart = "rpart",
  glm = "glm",
  rf = "rf",
  svm = "svmRadial"
)

# Perform nested cross-validation
results <- list()
for (model_name in names(models)) {
  model_method <- models[[model_name]]
  param_grid <- param_grids[[model_name]]
  
  # Perform nested cross-validation
  set.seed(123)
  nested_cv <- train(
    Attrition ~ ., data = train_data,
    method = model_method,
    tuneGrid = param_grid,
    trControl = outer_cv,
    metric = "Recall"
  )
  
  results[[model_name]] <- nested_cv
}

# Display results
results
```
### Re-Evaluation

```{r}
# Evaluate models on test data
evaluation_results <- list()

for (model_name in names(results)) {
  model <- results[[model_name]]
  
  # Generate predictions
  predictions <- predict(model, newdata = test_data)
  probabilities <- predict(model, newdata = test_data, type = "prob")[, 2]
  
  # Confusion matrix
  cm <- confusionMatrix(predictions, test_data$Attrition)
  print(paste("Confusion Matrix for", model_name))
  print(cm)
  
  # ROC curve
  roc_curve <- roc(test_data$Attrition, probabilities)
  print(paste("AUC for", model_name))
  print(auc(roc_curve))
  
  # Plot ROC curve
  plot(roc_curve, main = paste("ROC Curve for", model_name))
  
  # Store results
  evaluation_results[[model_name]] <- list(confusion_matrix = cm, roc_curve = roc_curve)
}

# Display evaluation results
evaluation_results
```
#### Try to improve Recall for Yes-Instances with Upsampling

```{r}
# Balance the training data using upsampling
set.seed(123)
train_data_balanced <- upSample(x = train_data[, -ncol(train_data)], y = train_data$Attrition)

# Define custom summary function to include recall
customSummary <- function(data, lev = NULL, model = NULL) {
  cm <- confusionMatrix(data$pred, data$obs, positive = lev[2])
  c(Recall = cm$byClass["Recall"], Specificity = cm$byClass["Specificity"], ROC = cm$byClass["ROC"])
}

# Define the control function for cross-validation
outer_cv <- trainControl(
  method = "cv",
  number = 5,
  savePredictions = "final",
  classProbs = TRUE,
  summaryFunction = customSummary
)

# Define the parameter grids for each model
param_grids <- list(
  knn = expand.grid(kmax = seq(3, 21, by = 2), distance = 2, kernel = "optimal"),
  rpart = expand.grid(cp = seq(0.01, 0.1, by = 0.01)),
  glm = NULL,
  rf = expand.grid(mtry = seq(2, 10, by = 2)),
  svm = expand.grid(C = 2^(2:4), sigma = 2^(-1:-3))
)

# Models list with method names as used in caret package
models <- list(
  knn = "kknn",
  rpart = "rpart",
  glm = "glm",
  rf = "rf",
  svm = "svmRadial"
)

# Perform nested cross-validation
results <- list()
for (model_name in names(models)) {
  model_method <- models[[model_name]]
  param_grid <- param_grids[[model_name]]
  
  # Perform nested cross-validation
  set.seed(123)
  nested_cv <- train(
    Class ~ ., data = train_data_balanced,
    method = model_method,
    tuneGrid = param_grid,
    trControl = outer_cv,
    metric = "Recall"
  )
  
  results[[model_name]] <- nested_cv
}

# Evaluate models on test data
evaluation_results <- list()

for (model_name in names(results)) {
  model <- results[[model_name]]
  
  # Generate predictions
  predictions <- predict(model, newdata = test_data)
  probabilities <- predict(model, newdata = test_data, type = "prob")[, 2]
  
  # Confusion matrix
  cm <- confusionMatrix(predictions, test_data$Attrition, positive = "Yes")
  print(paste("Confusion Matrix for", model_name))
  print(cm)
  
  # ROC curve
  roc_curve <- roc(test_data$Attrition, probabilities, levels = c("No", "Yes"), direction = "<")
  print(paste("AUC for", model_name))
  print(auc(roc_curve))
  
  # Plot ROC curve
  plot(roc_curve, main = paste("ROC Curve for", model_name))
  
  # Store results
  evaluation_results[[model_name]] <- list(confusion_matrix = cm, roc_curve = roc_curve)
}

# Display evaluation results
evaluation_results
```
### Ensemble Learning to further improve Recall
```{r}
# Install and load required packages
install.packages("ada")
library(ada)

# Perform threshold adjustment for glm model
glm_model <- results$glm
probabilities <- predict(glm_model, newdata = test_data, type = "prob")[, 2]
predictions <- ifelse(probabilities > 0.3, "Yes", "No")  # Adjusting the threshold to 0.3
confusionMatrix(as.factor(predictions), test_data$Attrition, positive = "Yes")

# Implement AdaBoost
set.seed(123)
adaboost_model <- ada(
  Class ~ ., data = train_data_balanced,
  iter = 50,
  type = "discrete"
)

# Evaluate AdaBoost model
predictions <- predict(adaboost_model, newdata = test_data)
probabilities <- predict(adaboost_model, newdata = test_data, type = "prob")[, 2]
cm <- confusionMatrix(predictions, test_data$Attrition, positive = "Yes")
roc_curve <- roc(test_data$Attrition, probabilities, levels = c("No", "Yes"), direction = "<")
print(cm)
print(auc(roc_curve))

```



